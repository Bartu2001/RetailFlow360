# RetailFlow360 â€“ GCP Data Engineering Case Study ğŸš€

This project simulates a modern retail data platform using Google Cloud tools.  
The objective is to create an end-to-end batch + streaming pipeline with proper transformations and a final dashboard layer.

## ğŸ’¼ Business Case

A large retail chain wants to unify sales data from various sources and make real-time data-driven decisions. However, current data is scattered in different systems, making analytics slow and ineffective.

## ğŸ”§ Tools Used

- **PostgreSQL** â†’ Simulates relational sales data source
- **MinIO** â†’ S3-compatible data lake for CSV files
- **Airbyte** â†’ Ingests data from PostgreSQL/MinIO to BigQuery
- **BigQuery** â†’ Cloud Data Warehouse for analytics
- **DBT** â†’ Data transformation (staging, intermediate, marts)
- **Python** â†’ Simulated real-time Pub/Sub data
- **Looker Studio** â†’ Dashboard layer
- **Airflow (Mocked)** â†’ Orchestrator, shows automation design

## ğŸ“ Folder Structure

- `dbt/` â†’ Contains staging, intermediate, and marts models
- `data/` â†’ All raw CSV data files
- `minio_data/` â†’ Mounted by MinIO
- `creds/` â†’ GCP service account key (gitignored)
- `stream/` â†’ Python scripts for publishing and subscribing to Pub/Sub
- `airflow/` â†’ DAG file and simulated log
- `.env`, `docker-compose.yml` â†’ Environment and orchestration setup

## ğŸ“Š Dashboards

Created using **Looker Studio**, showcasing:
- Sales performance
- Low stock alerts
- IoT sensor activity
- Marketplace vs. in-store comparison

## âš™ï¸ How to Run (Optional for GitHub Viewers)

This project is designed to simulate a real pipeline, so no actual GCP billing or secrets are required.

